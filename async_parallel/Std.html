<div class="ocaml_toplevel_module">
<div class="info"><code class="code">Parallel</code> is a library for running tasks in other processes on a cluster of machines.
    At its simplest, it exposes the <code class="code">Parallel.run</code> function:<br/>    |  val run : ?where:<code class="code">`Local | `On of string | `Random | `Random_on of string list</code>
         -&gt; (unit -&gt; 'a Deferred.t) -&gt; ('a, string) Result.t Deferred.t<br/>    where <code class="code">run f</code> creates another process on the machine specified by <code class="code">where</code> whose sole
    job is to compute <code class="code">f ()</code>.  The process that calls <code class="code">f</code> will receive the result of <code class="code">f
    ()</code> when it finishes.  Note that <code class="code">f</code> itself could call <code class="code">run</code>, thus allowing an
    arbitrarily nested processes across arbitrary machines in the cluster.<br/>    In order to use, <code class="code">Parallel.run</code>, for technical reasons, one must first call
    <code class="code">Parallel.init</code>, which must be called before any threads are created and before
    Async's scheduler is started.<br/>    Parallel's &quot;hubs&quot; and &quot;channels&quot; support typed bidirectional communication of streams
    of data between process.  One process creates a hub and listens to it.  Any other
    processes can then open a channel to the hub, and write values on the channel that
    will be received by the process listening to the hub.  Similarly, the hub process can
    send values via a channel to the process that opened the channel.<br/>    Moreover, channels may be passed between processes, either implicitly by being
    captured in a closure, or explicitly over another channel.<br/>    Implementation overview
    =======================
    There are three kinds of processes involved in a program the uses Parallel:<br/>    - the main process<ul><li>the master process</li><li>worker processes</li></ul>    Parallel dynamically creates a worker process to service each call to <code class="code">run</code>.<br/>    The OS process tree looks like:<br/>    | main
    |    master
    |      worker1
    |      ...
    |      workerN<br/>    As far as the OS is concerned, all workers are children of the master.  However, from
    the perspective of Parallel, the topology is more structured.  Each worker process is
    created on behalf of its &quot;owner&quot; process, which is either the main process or another
    worker process.  One can think of the main and worker processes as arranged in a tree
    different than the OS tree, in which there is an edge from each process to its owner
    (the main process has no owner).<br/>    Parallel uses OCaml's <code class="code">Marshal</code> library to serialize OCaml values to and from strings
    so that they can be sent over unix sockets between processes.  For example, the <code class="code">f</code>
    supplied to <code class="code">run</code> is marshalled and sent from the process that calls <code class="code">run</code> to the
    worker process that will run <code class="code">f</code>. Most, but not all values can be marshaled. Examples
    of values that can't be marshaled include C allocated abstract tagged values, custom
    blocks with no serilize/deserialize method.<br/>    The main process and all worker processes have a socket connected to the master
    process.  The master process's sole job is to service requests that are sent to
    these sockets, which can ask it to:<br/>    - create a new &quot;worker process&quot;, via <code class="code">create_process</code><br/>    As the master process receives requests, it does what each request asks, and then
    sends a response back via the socket to the client that made the request.<br/>    Each worker process has a socket connected to its owner process.  This socket
    initially receives the <code class="code">f</code> that the worker is to run, and is ultimately used to send
    the result back from the worker to the owner.<br/>    Here are the steps involved in implementing <code class="code">run f</code>.  There are three processes
    involved.<br/>    - R = the process calling <code class="code">run</code><ul><li>M = the master process</li><li>W = the worker process running the task</li></ul>    The steps are:<br/>    1. R asks M to create W
    2. M forks W
    3. M tells R about W
    4. R sends <code class="code">f</code> to W to run
    5. W runs <code class="code">f</code>
    6. W sends the result of <code class="code">f</code> to R
    7. M notices W has exited, and cleans up<br/>    When there are multiple machines in a cluster, each machine has a master process,
    and all the workers know about all master processes. When a worker wants to run on
    machine M, it looks up the address of that machine's master process in its table
    before performing step 1, everything after that is exactly the same as the example.<br/>    Notes:<br/>    Channel Passing
    ---------------<br/>    When a channel is passed from one process to another, the open socket is not actually
    passed. The API makes this pretty transparant, any api call will reconnect the
    channel, but it is useful to be aware of what is really going on as if you aren't
    aware you may create a race condition. For example, if I spawn a worker connected to a
    hub I have, and then I immediatly send something, it may or may not arrive, because
    the worker may not have time to connect and recieve it. A better strategy is to wait
    for the worker to say hello, and then send the data. This also means that you might
    have created only one channel from a given hub, but you can end up with as many
    connections (client ids) as workers who got hold of that channel. You can address them
    all individually, or you can always use <code class="code">send_to_all</code> if you really want to model a
    hub as a kind of shared bus.<br/>    Stdout and Stderr
    -----------------<br/>    Care has been taken to make printf style debugging work transparantly with parallel,
    even when run on a multiple machine cluster, stdout and stderr will be forwarded back
    to the master machine. This can cause some interleaving if you print a lot of
    messages, but generally works reasonably well (and we read and write in big chunks, so
    most of the interleaving won't be interline).<br/>    Some things to avoid marshaling
    -------------------------------<br/>    Monitor.t, Pcre.regexp, Writer.t, Reader.t, and similar kinds of objects shouldn't be
    depended upon to marshal correctly. Pcre.regexp is just right out, it definitly won't
    work. Monitor.t, Writer.t, and Reader.t, because of their complex nature, generally
    tow the entire async scheduler along with them, and because of that they will fail if
    any job on the scheduler queue has a custom object (e.g. regexp, or other C object)
    that can't be marshaled. You also can't marshal functions you've dynamically loaded
    (e.g. with ocaml plugin).<br/>    Processes don't share memory
    ----------------------------<br/>    The library can make it look very transparant to create and use other processes, but
    please remember these can literally be on some other machine maybe halfway round the
    earth. Global variables you set in one worker process have no effect whatsoever on
    other worker processes. I've personally come to believe that this is good, it results
    in better designed, more scalable systems.<br/>    Big shared things
    -----------------<br/>    Because of the way parallel works, with the master process an image of a very early
    state of one's program and workers forked from the master, it is usually not possible
    to share big static things in the way one might do in C using fork. Moreover, it isn't
    necessarially a win as you might think, if you know about how unix only copies pages
    on write when a process forks, you know that it should be a win. But the garbage
    collector ruins that completely, because as it scans it will write to EVERY page,
    causing a copy on write fault to copy the page, so you'll end up with a non shared
    copy of that big static thing in every process anyway. The best you can probably do is
    have one process own it and expose it with a query interface. Moreover, if you're
    running on multiple machines that IS the best you can do, so may as well get used to
    it.<br/>    Why Not Just Fork!?
    -------------------<br/>    The unix savvy amoung you may ask, what the heck are you doing with master processes
    and closure passing, just fork! Oh how that would make life easier, but alas, it
    really isn't possible. Why? You can't write async without threads, because the Unix
    API doesn't provide an asynchronous system call for every operation, meaning if you
    need to do something that might block, you must do it in a thread. And the list of
    stuff that might block is long and crippling. Want to read from a file without
    blocking out for SECONDS? Sorry! Not without a thread you don't. But once you've
    started a thread, all bets are off if you fork. POSIX actually doesn't even say
    anything about what happens to threads in a process that forks (besides saying they
    don't think its a good idea). In every sane OS, only the forking thread continues in
    the child, all the other threads are dead. OK, fine you say, let them die. But their
    mutexes, semephores and condition variables are in whatever state they were in the
    moment you forked, that is to say, any state at all. Unfortunatly this means that
    having created a thread that does anything meaningful (e.g. calls into libc), if you
    fork, all bets are off as to what happens in that child process. A dead thread may,
    for example, hold the lock around the C heap, which would mean that any call into libc
    would deadlock trying to allocate memory (oops), that'd ruin your day. Trust me, if
    parallel could work in some simpler way it would!<br/>    Say I Want To Have a Giant Shared Matrix
    ----------------------------------------<br/>    The parallelism model implemented is strictly message passing, shared memory isn't
    implemented, but there are various hacks you could use to make this work
    (e.g. implement it yourself). Bigarray already allowes mmaping files, so in theory
    even a cluster of machines could all mmap a giant file and use/mutate it.<br/>    Making Your Program Work on Multiple Machines
    ---------------------------------------------<br/>    The library makes this pretty transparant, however there are a couple of things to
    watch out for if you want it to work seamlessly. First of all, your program should be
    able to run with no arguments. Ideally you'd call Parallel.init before parsing
    arguments, or at least you'd check to see if you're a worker machine before parsing
    arguments. The reason for this is that the library is going to copy your program to
    every machine in the cluster and start it up, it's going to set an environment
    variable, and Parallel.init is going to check that environment variable and do
    something completely different if it's set, in fact Parallel.init will never return in
    this scenario, but will instead become the master process for that machine. If you
    want to change your behavior based on whether you're running a worker machine or the
    master you can use Parallel.is_worker_machine. In general put Parallel.init as early
    in your program as possible.<br/>    Try to avoid printing crazy things like sexps, or tons of data to stdout before
    calling Parallel.init. It uses stdout to communicate its address back to the master
    machine. The parser is pretty robust, and will toss out most things you print, but if
    you happen to print just the right sexp, it might think you're at the wrong
    address. This would just cause startup to hang, but would probably be hard to debug.<br/>    The examples (in the examples directory) all work on multiple machines, if you're
    stumped for a template to follow.<br/>    Why Can't I Use Async Before Parallel.init?
    -------------------------------------------<br/>    By default Parallel.init does a check that you haven't created any threads, and that
    you haven't made any use of async. The threads check is mandatory, but the async check
    can be turned off by setting <code class="code">fail_if_async_has_been_initialized</code> to false. Why is
    this check the default? Well in general you can't initialize async libraries before
    calling Parallel.init and expect them to work in the child process. The reason is that
    the async scheduler is thrown away in the child process before calling
    Scheduler.go. This is out of necessity, there is no way we can know what state the
    scheduler is in at the moment we fork, and it would be quite unfortunate if it were in
    a bad state, or worse, there are jobs on the queue that get run in all workers as soon
    as they call Scheduler.go. But as a result of this, any asyncy thing you create before
    Parallel.init won't work in worker processes. For example, say you initialize the log
    module before Parallel.init expecting to use it in the workers. It won't work, since
    all of its state (loops, writers, etc) is invalid in the worker processes. The check
    exists to make sure people are aware of this, and to make sure it only happens if they
    really know it's ok.<br/>    What CWD Will Worker Machine Processes Have?
    --------------------------------------------<br/>    If the CWD of the master exists on the worker machine, and you have permission to
    enter it, then parallel will switch to that directory before starting the master
    process, otherwise it will chdir to /.</div>
<div class="ocaml_module ident" name="Parallel" path="?package=async_parallel&amp;amp;module=Intf">
		    <pre><span class="keyword">module</span> <a href="?package=async_parallel&amp;module=Std.Parallel">Parallel</a> : <code class="code"><a href="?package=async_parallel&amp;module=Intf">Intf</a></code></pre>
</div>
<div class="ocaml_module ident" name="Channel" path="?package=async_parallel&amp;amp;module=Channel">
		    <pre><span class="keyword">module</span> <a href="?package=async_parallel&amp;module=Std.Channel">Channel</a> : <code class="code"><a href="?package=async_parallel&amp;module=Channel">Channel</a></code></pre>
</div>
<div class="ocaml_module ident" name="Hub" path="?package=async_parallel&amp;amp;module=Hub">
		    <pre><span class="keyword">module</span> <a href="?package=async_parallel&amp;module=Std.Hub">Hub</a> : <code class="code"><a href="?package=async_parallel&amp;module=Hub">Hub</a></code></pre>
</div>
<div class="ocaml_module ident" name="Cluster" path="?package=async_parallel&amp;amp;module=Import.Cluster">
		    <pre><span class="keyword">module</span> <a href="?package=async_parallel&amp;module=Std.Cluster">Cluster</a> : <code class="code"><a href="?package=async_parallel&amp;module=Import.Cluster">Import.Cluster</a></code></pre>
</div></div>